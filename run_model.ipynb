{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b11cbce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Subset\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc7c8a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import HierarchicalStockDataset, ShortTermSequentialLearner, IntraSectorGAT ,AttentiveGRU, LongTermSequentialLearner, InterSectorGAT, EmbeddingFusion, FinGAT, MultiTaskLoss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8716ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa4b33bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'fingat_model.pth'\n",
    "\n",
    "\n",
    "model = FinGAT(\n",
    "    attentive_dim=HIDDEN_SIZE,\n",
    "    graph_dim=HIDDEN_SIZE,\n",
    "    sector_dim=HIDDEN_SIZE\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f949326",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('merged_stock_data_with_enhanced_features.parquet')\n",
    "df.head()\n",
    "\n",
    "df.index = pd.to_datetime(df.index)  \n",
    "\n",
    "start_date = \"2024-01-01\"\n",
    "end_date = \"2024-03-22\"\n",
    "\n",
    "\n",
    "df_val_date  = df.loc[start_date:end_date]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9be3b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset contains 22491 samples\n",
      "Validation attentive embeddings processed: 441 companies\n",
      "\n",
      "--- Evaluating on Validation Data with Fresh Embeddings ---\n",
      "Creating validation embeddings from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gn/mtbdxhjn1697bp_qhyd6y0cr0000gn/T/ipykernel_71337/1140316538.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  graph_emb = torch.tensor(weekly_data[w]).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics (using fresh embeddings):\n",
      "Number of validation samples: 441\n",
      "Return prediction correlation: 0.0017\n",
      "Movement prediction accuracy: 0.4104\n",
      "\n",
      "Mean Reciprocal Rank (MRR) Metrics:\n",
      "MRR@5: 0.1034\n",
      "MRR@10: 0.0887\n",
      "MRR@20: 0.0486\n",
      "MRR@30: 0.0348\n",
      "MRR@100: 0.0174\n",
      "\n",
      "Information Ratio (IRR) Metrics:\n",
      "IRR@5: 1.0446 (Excess Return: 0.0415, Top-5 Avg: 0.0307, Bottom-5 Avg: -0.0109)\n",
      "IRR@10: 0.4018 (Excess Return: 0.0156, Top-10 Avg: 0.0138, Bottom-10 Avg: -0.0017)\n",
      "IRR@20: 0.2582 (Excess Return: 0.0101, Top-20 Avg: 0.0085, Bottom-20 Avg: -0.0016)\n",
      "IRR@30: 0.2040 (Excess Return: 0.0075, Top-30 Avg: 0.0069, Bottom-30 Avg: -0.0006)\n",
      "IRR@100: -0.0132 (Excess Return: -0.0004, Top-100 Avg: 0.0030, Bottom-100 Avg: 0.0034)\n",
      "\n",
      "--- Evaluation Complete ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_dataset = HierarchicalStockDataset(df_val_date)\n",
    "print(f\"Validation dataset contains {val_dataset.__len__()} samples\")\n",
    "\n",
    "val_company_sequences = defaultdict(list)\n",
    "sector_stock_map = defaultdict(set)\n",
    "stock_sector_map = defaultdict(set)\n",
    "\n",
    "for idx in range(len(val_dataset)):\n",
    "    features, industry_id, company_id, return_ratio, movements = val_dataset[idx]\n",
    "    company_id = company_id.item()\n",
    "    industry_id = industry_id.item()\n",
    "    \n",
    "    val_company_sequences[company_id].append({\n",
    "        'features': features,\n",
    "        'industry_id': industry_id,\n",
    "        'return_ratio': return_ratio.item(),\n",
    "        'movements': movements.item(),\n",
    "        'idx': idx\n",
    "    })\n",
    "    sector_stock_map[industry_id].add(company_id)\n",
    "    stock_sector_map[company_id].add(industry_id)\n",
    "\n",
    "\n",
    "val_attentive_embeddings = {}\n",
    "val_stock_returns_map = defaultdict(list)\n",
    "val_stock_movements_map = defaultdict(list)\n",
    "\n",
    "\n",
    "stsl_model = ShortTermSequentialLearner(\n",
    "    input_size=val_dataset[0][0].shape[1],  \n",
    "    hidden_size=HIDDEN_SIZE\n",
    ")\n",
    "stsl_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for company_id, sequences in val_company_sequences.items():\n",
    "\n",
    "        sequences.sort(key=lambda x: x['idx'])\n",
    "\n",
    "        company_attentive_embeddings = []\n",
    "        \n",
    "        for seq in sequences:\n",
    "            features = seq['features'].unsqueeze(0)\n",
    "            industry_id = seq['industry_id']\n",
    "            return_ratio = seq['return_ratio']\n",
    "            movements = seq['movements']\n",
    "\n",
    "            val_stock_returns_map[company_id].append(return_ratio)\n",
    "            val_stock_movements_map[company_id].append(movements)\n",
    "\n",
    "            context, _ = stsl_model(features)\n",
    "            company_attentive_embeddings.append(context.squeeze(0).cpu().numpy())\n",
    "\n",
    "        if company_attentive_embeddings:  \n",
    "            val_attentive_embeddings[company_id] = np.stack(company_attentive_embeddings, axis=0)\n",
    "print(f\"Validation attentive embeddings processed: {len(val_attentive_embeddings)} companies\")\n",
    "\n",
    "# Function to evaluate on validation data\n",
    "def evaluate_on_validation(trained_model):\n",
    "\n",
    "    print(\"Creating validation embeddings from scratch...\")\n",
    "    global val_weekly_long_term_embeddings, val_weekly_inter_sector_embeddings\n",
    "\n",
    "    val_weekly_intra_sector_graphs = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    max_val_weeks = max([len(emb) for emb in val_attentive_embeddings.values()]) if val_attentive_embeddings else 0\n",
    "\n",
    "    for week_idx in range(max_val_weeks):\n",
    "\n",
    "\n",
    "        for sector_id, sector_stocks in sector_stock_map.items():\n",
    "            sector_features = []\n",
    "            valid_indices = []\n",
    "\n",
    "            for stock_idx in sector_stocks:\n",
    "                if stock_idx in val_attentive_embeddings and week_idx < len(val_attentive_embeddings[stock_idx]):\n",
    "                    tensor_embedding = torch.tensor(val_attentive_embeddings[stock_idx][week_idx], dtype=torch.float32)\n",
    "                    sector_features.append(tensor_embedding)\n",
    "                    valid_indices.append(stock_idx)\n",
    "\n",
    "            if len(sector_features) >= 2:\n",
    "                edge_index = []\n",
    "                num_nodes = len(valid_indices)\n",
    "\n",
    "                for i in range(num_nodes):\n",
    "                    for j in range(num_nodes):\n",
    "                        if i != j:\n",
    "                            edge_index.append([i, j])\n",
    "\n",
    "                val_weekly_intra_sector_graphs[week_idx][sector_id] = Data(\n",
    "                    x=torch.stack(sector_features),\n",
    "                    edge_index=torch.tensor(edge_index).t().contiguous(),\n",
    "                    original_indices=valid_indices,\n",
    "                    sector_id=sector_id\n",
    "                )\n",
    "\n",
    "    val_sector_embeddings = []\n",
    "    gat_model = IntraSectorGAT()\n",
    "    gat_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for week_idx in val_weekly_intra_sector_graphs:\n",
    "            for sector_id, graph in val_weekly_intra_sector_graphs[week_idx].items():\n",
    "                graph = graph\n",
    "                out = gat_model(graph)\n",
    "                val_sector_embeddings.append({\n",
    "                    'embeddings': out.cpu(),\n",
    "                    'original_indices': graph.original_indices,\n",
    "                    'sector_id': graph.sector_id,\n",
    "                    'week_idx': week_idx\n",
    "                })\n",
    "\n",
    "    val_weekly_sector_pooled_embeddings = defaultdict(dict)\n",
    "    \n",
    "    if val_sector_embeddings:\n",
    "        embedding_dim = val_sector_embeddings[0]['embeddings'].shape[1]\n",
    "        \n",
    "        for week_idx in sorted(val_weekly_intra_sector_graphs.keys()):\n",
    "            for sector_id in val_weekly_intra_sector_graphs[week_idx]:\n",
    "                graph = val_weekly_intra_sector_graphs[week_idx][sector_id]\n",
    "                pooled_embedding = torch.max(graph.x, dim=0)[0]  \n",
    "                val_weekly_sector_pooled_embeddings[week_idx][sector_id] = pooled_embedding\n",
    "\n",
    "    val_weekly_inter_sector_embeddings = defaultdict(dict)\n",
    "    \n",
    "    if val_sector_embeddings:\n",
    "        inter_sector_model = InterSectorGAT(\n",
    "            in_channels=embedding_dim,\n",
    "            hidden_channels=HIDDEN_SIZE\n",
    "        )\n",
    "        inter_sector_model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for week_idx in sorted(val_weekly_sector_pooled_embeddings.keys()):\n",
    "                sectors_this_week = val_weekly_sector_pooled_embeddings[week_idx]\n",
    "\n",
    "                num_sectors = len(sectors_this_week)\n",
    "                if num_sectors < 2:\n",
    "                    continue\n",
    "\n",
    "                sector_ids = list(sectors_this_week.keys())\n",
    "                sector_id_to_idx = {sector_id: idx for idx, sector_id in enumerate(sector_ids)}\n",
    "\n",
    "                sector_features = torch.zeros((num_sectors, embedding_dim), dtype=torch.float32)\n",
    "\n",
    "                for sector_id, idx in sector_id_to_idx.items():\n",
    "                    sector_features[idx] = sectors_this_week[sector_id]\n",
    "\n",
    "                edge_indices = []\n",
    "                for i in range(num_sectors):\n",
    "                    for j in range(num_sectors):\n",
    "                        if i != j: \n",
    "                            edge_indices.append([i, j])\n",
    "                \n",
    "                edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "\n",
    "                sector_features = sector_features\n",
    "                edge_index = edge_index\n",
    "                inter_sector_embeddings = inter_sector_model(sector_features, edge_index)\n",
    "\n",
    "                inter_sector_embeddings = inter_sector_embeddings.cpu()\n",
    "                for sector_id, idx in sector_id_to_idx.items():\n",
    "                    val_weekly_inter_sector_embeddings[week_idx][sector_id] = inter_sector_embeddings[idx]\n",
    "\n",
    "    val_company_sector_embeddings = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    for sector_data in val_sector_embeddings:\n",
    "        week_idx = sector_data['week_idx']\n",
    "        original_indices = sector_data['original_indices']\n",
    "        embeddings = sector_data['embeddings']\n",
    "        \n",
    "        for i, company_idx in enumerate(original_indices):\n",
    "            val_company_sector_embeddings[company_idx][week_idx] = embeddings[i]\n",
    "\n",
    "    val_weekly_long_term_embeddings = defaultdict(lambda: defaultdict(dict))\n",
    "    SEQ_LENGTH = 4 \n",
    "    \n",
    "    if val_sector_embeddings:\n",
    "\n",
    "        long_term_g = LongTermSequentialLearner(\n",
    "            input_size=val_sector_embeddings[0]['embeddings'].shape[1],\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            lookback_weeks=SEQ_LENGTH\n",
    "        )\n",
    "        \n",
    "        long_term_a = LongTermSequentialLearner(\n",
    "            input_size=next(iter(val_attentive_embeddings.values())).shape[1],\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            lookback_weeks=SEQ_LENGTH\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            if val_company_sector_embeddings:\n",
    "                max_weeks = max([max(weekly_data.keys()) for weekly_data in val_company_sector_embeddings.values()])\n",
    "\n",
    "                for current_week in range(SEQ_LENGTH, max_weeks + 1):\n",
    "                    \n",
    "\n",
    "                    for company_idx, weekly_data in val_company_sector_embeddings.items():\n",
    "\n",
    "                        if current_week not in weekly_data:\n",
    "                            continue\n",
    "\n",
    "                        history_weeks = [w for w in range(current_week - SEQ_LENGTH, current_week)]\n",
    "                        if not all(w in weekly_data for w in history_weeks):\n",
    "                            continue\n",
    "\n",
    "                        graph_window = []\n",
    "                        for w in history_weeks:\n",
    "                            if w in weekly_data:\n",
    "                                graph_emb = torch.tensor(weekly_data[w]).unsqueeze(0)\n",
    "                                graph_window.append(graph_emb)\n",
    "\n",
    "                        if company_idx in val_attentive_embeddings:\n",
    "                            attentive_seqs = val_attentive_embeddings[company_idx]\n",
    "                            \n",
    "                            if len(attentive_seqs) >= SEQ_LENGTH:\n",
    "                                attentive_window = []\n",
    "                                for i in range(SEQ_LENGTH):\n",
    "                                    seq_idx = len(attentive_seqs) - SEQ_LENGTH + i\n",
    "                                    if seq_idx >= 0 and seq_idx < len(attentive_seqs):\n",
    "                                        att_emb = torch.tensor(attentive_seqs[seq_idx], dtype=torch.float32).unsqueeze(0)\n",
    "                                        attentive_window.append(att_emb)\n",
    "                                \n",
    "                                if len(graph_window) == SEQ_LENGTH and len(attentive_window) == SEQ_LENGTH:\n",
    "                                    tau_G = long_term_g(graph_window)\n",
    "                                    tau_A = long_term_a(attentive_window)\n",
    "\n",
    "                                    val_weekly_long_term_embeddings[current_week][company_idx] = {\n",
    "                                        'graph': tau_G.cpu().numpy(),\n",
    "                                        'attentive': tau_A.cpu().numpy()\n",
    "                                    }\n",
    "\n",
    "    val_data = {\n",
    "        'attentive_embs': [],\n",
    "        'graph_embs': [],\n",
    "        'sector_embs': [],\n",
    "        'returns': [],\n",
    "        'movements': [],\n",
    "        'companies': []\n",
    "    }\n",
    "    \n",
    "  \n",
    "    if val_weekly_long_term_embeddings:\n",
    "        last_week = max(val_weekly_long_term_embeddings.keys())\n",
    "\n",
    "        \n",
    "        for company_idx, company_data in val_weekly_long_term_embeddings[last_week].items():\n",
    "\n",
    "            if company_idx not in stock_sector_map or len(stock_sector_map[company_idx]) == 0:\n",
    "                continue\n",
    "                \n",
    "            sector_id = list(stock_sector_map[company_idx])[0]\n",
    "\n",
    "            if sector_id not in val_weekly_inter_sector_embeddings[last_week]:\n",
    "                continue\n",
    "                \n",
    "   \n",
    "            attentive_emb = torch.tensor(company_data['attentive'], dtype=torch.float32)\n",
    "            graph_emb = torch.tensor(company_data['graph'], dtype=torch.float32)\n",
    "            sector_emb = val_weekly_inter_sector_embeddings[last_week][sector_id]\n",
    "            \n",
    "       \n",
    "            if company_idx in val_stock_returns_map and val_stock_returns_map[company_idx]:\n",
    "                return_ratio = val_stock_returns_map[company_idx][0]\n",
    "                movement = 1.0 if return_ratio > 0 else 0.0\n",
    "                \n",
    "           \n",
    "                val_data['attentive_embs'].append(attentive_emb)\n",
    "                val_data['graph_embs'].append(graph_emb)\n",
    "                val_data['sector_embs'].append(sector_emb)\n",
    "                val_data['companies'].append(company_idx)\n",
    "                val_data['returns'].append(return_ratio)\n",
    "                val_data['movements'].append(movement)\n",
    "    \n",
    "    \n",
    "    if val_data['returns']:\n",
    "        val_data['attentive_embs'] = torch.stack(val_data['attentive_embs'])\n",
    "        val_data['graph_embs'] = torch.stack(val_data['graph_embs'])\n",
    "        val_data['sector_embs'] = torch.stack(val_data['sector_embs'])\n",
    "        val_data['returns'] = torch.tensor(val_data['returns'], dtype=torch.float32)\n",
    "        val_data['movements'] = torch.tensor(val_data['movements'], dtype=torch.float32)\n",
    "        \n",
    "    \n",
    "        trained_model.eval()\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            attentive_embs = val_data['attentive_embs']\n",
    "            graph_embs = val_data['graph_embs']\n",
    "            sector_embs = val_data['sector_embs']\n",
    "\n",
    "            return_preds, movement_preds = trained_model(attentive_embs, graph_embs, sector_embs)\n",
    "   \n",
    "            return_preds_np = return_preds.cpu().numpy()\n",
    "            return_targets_np = val_data['returns'].numpy()\n",
    "            movement_preds_np = movement_preds.cpu().numpy()\n",
    "            movement_targets_np = val_data['movements'].numpy()\n",
    "            \n",
    " \n",
    "            from scipy.stats import spearmanr\n",
    "            corr, _ = spearmanr(return_preds_np, return_targets_np)\n",
    "  \n",
    "            threshold = 0.5\n",
    "            binary_preds = [1 if p > threshold else 0 for p in movement_preds_np]\n",
    "            accuracy = sum(p == t for p, t in zip(binary_preds, movement_targets_np)) / len(binary_preds)\n",
    " \n",
    "            from sklearn.metrics import r2_score\n",
    "            r2 = r2_score(return_targets_np, return_preds_np)\n",
    "   \n",
    "            pred_target_pairs = list(zip(return_preds_np, return_targets_np, val_data['companies']))\n",
    "            pred_ranking = sorted(pred_target_pairs, key=lambda x: x[0], reverse=True)\n",
    "            company_to_pred_rank = {company: i+1 for i, (_, _, company) in enumerate(pred_ranking)}\n",
    "            \n",
    "            true_ranking = sorted(\n",
    "                [(target, company) for (_, target, company) in pred_target_pairs],\n",
    "                key=lambda x: x[0], \n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            mrr_values = {}\n",
    "            top_k_values = [5, 10, 20, 30, 100]\n",
    "            \n",
    "            for k in top_k_values:\n",
    "                mrr_at_k = []\n",
    "                for j, (_, company) in enumerate(true_ranking[:min(k, len(true_ranking))]):\n",
    "                    if company in company_to_pred_rank:\n",
    "                        mrr_at_k.append(1.0 / company_to_pred_rank[company])\n",
    "                \n",
    "                mrr_values[k] = sum(mrr_at_k) / len(mrr_at_k) if mrr_at_k else 0\n",
    "\n",
    "            print(f\"\\nValidation Metrics (using fresh embeddings):\")\n",
    "            print(f\"Number of validation samples: {len(return_preds_np)}\")\n",
    "            print(f\"Return prediction correlation: {corr:.4f}\")\n",
    "            print(f\"Movement prediction accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "            \n",
    "            print(\"\\nMean Reciprocal Rank (MRR) Metrics:\")\n",
    "            for k in top_k_values:\n",
    "                print(f\"MRR@{k}: {mrr_values[k]:.4f}\")\n",
    "            irr_values = {}\n",
    "            top_k_values = [5, 10, 20, 30, 100]  # Simplified: removed unnecessary conditional\n",
    "\n",
    "            for k in top_k_values:\n",
    "                # For each window size, sort predictions and get top and bottom k stocks\n",
    "                pred_sorted_indices = np.argsort(return_preds_np)[::-1]  # FIXED: Use return_preds_np\n",
    "                \n",
    "                # Get actual returns for top k and bottom k predicted stocks\n",
    "                if len(pred_sorted_indices) >= k*2:\n",
    "                    top_k_returns = [return_targets_np[i] for i in pred_sorted_indices[:k]]  # FIXED: Use return_targets_np\n",
    "                    bottom_k_returns = [return_targets_np[i] for i in pred_sorted_indices[-k:]]\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    top_k_mean = np.mean(top_k_returns)\n",
    "                    bottom_k_mean = np.mean(bottom_k_returns)\n",
    "                    excess_return = top_k_mean - bottom_k_mean\n",
    "                    \n",
    "                    # Calculate tracking error as the standard deviation of return differences\n",
    "                    if len(top_k_returns) > 1:\n",
    "                        tracking_error = np.std(np.array(top_k_returns) - np.array(bottom_k_returns))\n",
    "                        ir = excess_return / tracking_error if tracking_error > 0 else 0\n",
    "                    else:\n",
    "                        ir = 0\n",
    "                        \n",
    "                    irr_values[k] = {\n",
    "                        'ir': ir,\n",
    "                        'top_returns': top_k_mean,\n",
    "                        'bottom_returns': bottom_k_mean,\n",
    "                        'excess_return': excess_return\n",
    "                    }\n",
    "\n",
    "            # Print IRR metrics\n",
    "            print(\"\\nInformation Ratio (IRR) Metrics:\")\n",
    "            for k in top_k_values:\n",
    "                if k in irr_values:\n",
    "                    print(f\"IRR@{k}: {irr_values[k]['ir']:.4f} (Excess Return: {irr_values[k]['excess_return']:.4f}, \"\n",
    "                        f\"Top-{k} Avg: {irr_values[k]['top_returns']:.4f}, Bottom-{k} Avg: {irr_values[k]['bottom_returns']:.4f})\")\n",
    "                        \n",
    "            return corr, accuracy, r2, mrr_values\n",
    "    else:\n",
    "        print(\"No validation data available for evaluation\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "\n",
    "print(\"\\n--- Evaluating on Validation Data with Fresh Embeddings ---\")\n",
    "val_metrics = evaluate_on_validation(model)\n",
    "print(\"\\n--- Evaluation Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef13750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions_csv_with_names(model):\n",
    "    print(\"Generating predictions for each company across all dates...\")\n",
    "    \n",
    "    # Create a mapping from company_id to company name\n",
    "    # Assuming the DataFrame has columns for company ID and name (like 'symbol' or 'company_name')\n",
    "    # If your DataFrame columns are different, adjust accordingly\n",
    "    company_id_to_name = {}\n",
    "    \n",
    "    # Choose one of these approaches based on your DataFrame structure:\n",
    "    \n",
    "    # Option 1: If company identifiers are in the index and names are in a column\n",
    "    if 'company_name' in df_val_date.columns:\n",
    "        for idx, row in df_val_date.reset_index().drop_duplicates(['company_id']).iterrows():\n",
    "            company_id_to_name[row['company_id']] = row['company_name']\n",
    "    \n",
    "    # Option 2: If there's a 'symbol' or 'ticker' column that has readable names\n",
    "    elif 'symbol' in df_val_date.columns:\n",
    "        for idx, row in df_val_date.reset_index().drop_duplicates(['company_id']).iterrows():\n",
    "            company_id_to_name[row['company_id']] = row['symbol']\n",
    "    \n",
    "    # Default: If no name column is found, use the IDs but convert to string\n",
    "    if not company_id_to_name:\n",
    "        print(\"Warning: Could not find company names in the dataset. Using IDs.\")\n",
    "        # Will use str(company_id) as fallback\n",
    "    \n",
    "    predictions_data = {\n",
    "        'date': [],\n",
    "        'company_id': [],\n",
    "        'company_name': [],  # New column for company names\n",
    "        'actual_return': [],\n",
    "        'predicted_return': [],\n",
    "        'movement_actual': [],\n",
    "        'movement_predicted': []\n",
    "    }\n",
    "    \n",
    "    if val_weekly_long_term_embeddings:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for week_idx in sorted(val_weekly_long_term_embeddings.keys()):\n",
    "                try:\n",
    "                    dates = df_val_date.index.unique()\n",
    "                    if week_idx < len(dates):\n",
    "                        current_date = dates[week_idx]\n",
    "                    else:\n",
    "                        current_date = pd.Timestamp(end_date)\n",
    "                except:\n",
    "                    current_date = pd.Timestamp(start_date) + pd.Timedelta(days=7*week_idx)\n",
    "                \n",
    "                for company_idx, company_data in val_weekly_long_term_embeddings[week_idx].items():\n",
    "                    if (company_idx not in stock_sector_map or \n",
    "                        len(stock_sector_map[company_idx]) == 0):\n",
    "                        continue\n",
    "                    \n",
    "                    sector_id = list(stock_sector_map[company_idx])[0]\n",
    "                    if sector_id not in val_weekly_inter_sector_embeddings[week_idx]:\n",
    "                        continue\n",
    "                    \n",
    "                    attentive_emb = torch.tensor(company_data['attentive'], dtype=torch.float32).unsqueeze(0)\n",
    "                    graph_emb = torch.tensor(company_data['graph'], dtype=torch.float32).unsqueeze(0)\n",
    "                    sector_emb = val_weekly_inter_sector_embeddings[week_idx][sector_id].unsqueeze(0)\n",
    "                    \n",
    "                    if company_idx in val_stock_returns_map and len(val_stock_returns_map[company_idx]) > week_idx:\n",
    "                        actual_return = val_stock_returns_map[company_idx][week_idx]\n",
    "                        actual_movement = 1.0 if actual_return > 0 else 0.0\n",
    "                        \n",
    "                        return_pred, movement_pred = model(attentive_emb, graph_emb, sector_emb)\n",
    "                        \n",
    "                        # Get company name from mapping, or use ID as string if not found\n",
    "                        company_name = company_id_to_name.get(company_idx, f\"Company_{company_idx}\")\n",
    "                        \n",
    "                        predictions_data['date'].append(current_date)\n",
    "                        predictions_data['company_id'].append(company_idx)\n",
    "                        predictions_data['company_name'].append(company_name)\n",
    "                        predictions_data['actual_return'].append(actual_return)\n",
    "                        predictions_data['predicted_return'].append(return_pred.item())\n",
    "                        predictions_data['movement_actual'].append(actual_movement)\n",
    "                        predictions_data['movement_predicted'].append(movement_pred.item())\n",
    "    \n",
    "    if predictions_data['date']:\n",
    "        pred_df = pd.DataFrame(predictions_data)\n",
    "        \n",
    "        # Convert company_id to string for better readability\n",
    "        pred_df['company_id'] = pred_df['company_id'].astype(str)\n",
    "        \n",
    "        output_file = 'stock_return_predictions.csv'\n",
    "        pred_df.to_csv(output_file, index=False)\n",
    "        print(f\"Predictions saved to {output_file}\")\n",
    "        \n",
    "        print(\"\\nSample predictions:\")\n",
    "        print(pred_df.head())\n",
    "        \n",
    "        return pred_df\n",
    "    else:\n",
    "        print(\"No prediction data available to save\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d68f2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for each company across all dates...\n",
      "Warning: Could not find company names in the dataset. Using IDs.\n",
      "Predictions saved to stock_return_predictions.csv\n",
      "\n",
      "Sample predictions:\n",
      "                       date company_id company_name  actual_return  \\\n",
      "0 2024-01-05 00:00:00+05:30        131  Company_131      -0.018340   \n",
      "1 2024-01-05 00:00:00+05:30        136  Company_136      -0.035540   \n",
      "2 2024-01-05 00:00:00+05:30        392  Company_392      -0.004899   \n",
      "3 2024-01-05 00:00:00+05:30        270  Company_270       0.012167   \n",
      "4 2024-01-05 00:00:00+05:30        400  Company_400       0.026711   \n",
      "\n",
      "   predicted_return  movement_actual  movement_predicted  \n",
      "0         -0.156015              0.0            0.488052  \n",
      "1         -0.155235              0.0            0.491528  \n",
      "2         -0.155392              0.0            0.491395  \n",
      "3         -0.156076              1.0            0.488107  \n",
      "4         -0.156069              1.0            0.488050  \n",
      "\n",
      "Total predictions generated: 20727\n",
      "Number of unique companies: 441\n",
      "Date range: 2024-01-05 00:00:00+05:30 to 2024-03-14 00:00:00+05:30\n"
     ]
    }
   ],
   "source": [
    "predictions_df = generate_predictions_csv_with_names(model)\n",
    "\n",
    "if predictions_df is not None:\n",
    "    print(f\"\\nTotal predictions generated: {len(predictions_df)}\")\n",
    "    print(f\"Number of unique companies: {predictions_df['company_id'].nunique()}\")\n",
    "    print(f\"Date range: {predictions_df['date'].min()} to {predictions_df['date'].max()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Universal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
