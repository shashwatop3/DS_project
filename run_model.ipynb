{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b11cbce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Subset\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bc7c8a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import HierarchicalStockDataset, TransformerSequentialLearner, IntraSectorGAT, LongTermTransformerLearner, EmbeddingFusion, FinGAT, MultiTaskLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8716ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "aa4b33bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'fingat_tranformer_model.pth'\n",
    "\n",
    "\n",
    "model = FinGAT(\n",
    "    attentive_dim=HIDDEN_SIZE,\n",
    "    graph_dim=HIDDEN_SIZE,\n",
    "    sector_dim=HIDDEN_SIZE\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9f949326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 10680)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('stock_data/processed/merged_stock_data_with_enhanced_features.parquet')\n",
    "df.head()\n",
    "\n",
    "df.index = pd.to_datetime(df.index)  \n",
    "\n",
    "start_date = \"2025-01-11\"\n",
    "end_date = \"2025-03-22\"\n",
    "\n",
    "\n",
    "df_val_date  = df.loc[start_date:end_date]\n",
    "print(df_val_date.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c9be3b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset contains 19135 samples\n",
      "Test attentive embeddings processed: 445 companies\n",
      "\n",
      "--- Evaluating on Test Data with Fresh Embeddings ---\n",
      "Creating test embeddings from scratch...\n",
      "Processing test week 0\n",
      "Processing test week 1\n",
      "Processing test week 2\n",
      "Processing test week 3\n",
      "Processing test week 4\n",
      "Processing test week 5\n",
      "Processing test week 6\n",
      "Processing test week 7\n",
      "Processing test week 8\n",
      "Processing test week 9\n",
      "Processing test week 10\n",
      "Processing test week 11\n",
      "Processing test week 12\n",
      "Processing test week 13\n",
      "Processing test week 14\n",
      "Processing test week 15\n",
      "Processing test week 16\n",
      "Processing test week 17\n",
      "Processing test week 18\n",
      "Processing test week 19\n",
      "Processing test week 20\n",
      "Processing test week 21\n",
      "Processing test week 22\n",
      "Processing test week 23\n",
      "Processing test week 24\n",
      "Processing test week 25\n",
      "Processing test week 26\n",
      "Processing test week 27\n",
      "Processing test week 28\n",
      "Processing test week 29\n",
      "Processing test week 30\n",
      "Processing test week 31\n",
      "Processing test week 32\n",
      "Processing test week 33\n",
      "Processing test week 34\n",
      "Processing test week 35\n",
      "Processing test week 36\n",
      "Processing test week 37\n",
      "Processing test week 38\n",
      "Processing test week 39\n",
      "Processing test week 40\n",
      "Processing test week 41\n",
      "Processing test week 42\n",
      "Creating new GATConv model for input dimension 16\n",
      "Processing validation long-term embeddings for week 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gn/mtbdxhjn1697bp_qhyd6y0cr0000gn/T/ipykernel_84453/713020245.py:278: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  graph_emb = torch.tensor(weekly_data[w]).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation long-term embeddings for week 5\n",
      "Processing validation long-term embeddings for week 6\n",
      "Processing validation long-term embeddings for week 7\n",
      "Processing validation long-term embeddings for week 8\n",
      "Processing validation long-term embeddings for week 9\n",
      "Processing validation long-term embeddings for week 10\n",
      "Processing validation long-term embeddings for week 11\n",
      "Processing validation long-term embeddings for week 12\n",
      "Processing validation long-term embeddings for week 13\n",
      "Processing validation long-term embeddings for week 14\n",
      "Processing validation long-term embeddings for week 15\n",
      "Processing validation long-term embeddings for week 16\n",
      "Processing validation long-term embeddings for week 17\n",
      "Processing validation long-term embeddings for week 18\n",
      "Processing validation long-term embeddings for week 19\n",
      "Processing validation long-term embeddings for week 20\n",
      "Processing validation long-term embeddings for week 21\n",
      "Processing validation long-term embeddings for week 22\n",
      "Processing validation long-term embeddings for week 23\n",
      "Processing validation long-term embeddings for week 24\n",
      "Processing validation long-term embeddings for week 25\n",
      "Processing validation long-term embeddings for week 26\n",
      "Processing validation long-term embeddings for week 27\n",
      "Processing validation long-term embeddings for week 28\n",
      "Processing validation long-term embeddings for week 29\n",
      "Processing validation long-term embeddings for week 30\n",
      "Processing validation long-term embeddings for week 31\n",
      "Processing validation long-term embeddings for week 32\n",
      "Processing validation long-term embeddings for week 33\n",
      "Processing validation long-term embeddings for week 34\n",
      "Processing validation long-term embeddings for week 35\n",
      "Processing validation long-term embeddings for week 36\n",
      "Processing validation long-term embeddings for week 37\n",
      "Processing validation long-term embeddings for week 38\n",
      "Processing validation long-term embeddings for week 39\n",
      "Processing validation long-term embeddings for week 40\n",
      "Processing validation long-term embeddings for week 41\n",
      "Processing validation long-term embeddings for week 42\n",
      "Using week 42 for final evaluation\n",
      "\n",
      "Validation Metrics (using transformer model):\n",
      "Number of validation samples: 445\n",
      "Return prediction correlation: 0.0670\n",
      "Movement prediction accuracy: 0.2180\n",
      "\n",
      "Mean Reciprocal Rank (MRR) Metrics:\n",
      "MRR@5: 0.0054\n",
      "MRR@10: 0.0064\n",
      "MRR@20: 0.0075\n",
      "MRR@30: 0.0396\n",
      "MRR@100: 0.0237\n",
      "\n",
      "Information Ratio (IRR) Metrics:\n",
      "IRR@5: 1.1525 (Excess Return: 0.0186, Top-5 Avg: -0.0030, Bottom-5 Avg: -0.0216)\n",
      "IRR@10: 0.6550 (Excess Return: 0.0090, Top-10 Avg: -0.0043, Bottom-10 Avg: -0.0134)\n",
      "IRR@20: 0.3248 (Excess Return: 0.0079, Top-20 Avg: -0.0122, Bottom-20 Avg: -0.0201)\n",
      "IRR@30: -0.0065 (Excess Return: -0.0002, Top-30 Avg: -0.0187, Bottom-30 Avg: -0.0185)\n",
      "IRR@100: 0.0894 (Excess Return: 0.0023, Top-100 Avg: -0.0152, Bottom-100 Avg: -0.0176)\n",
      "Mean Absolute Error (MAE): 0.376897\n",
      "\n",
      "--- Evaluation Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Create the validation dataset\n",
    "val_dataset = HierarchicalStockDataset(df_val_date)\n",
    "print(f\"Test dataset contains {val_dataset.__len__()} samples\")\n",
    "\n",
    "# Process validation data similarly to training data\n",
    "val_company_sequences = defaultdict(list)\n",
    "sector_stock_map = defaultdict(set)\n",
    "stock_sector_map = defaultdict(set)\n",
    "\n",
    "# Organize validation samples by company\n",
    "for idx in range(len(val_dataset)):\n",
    "    features, industry_id, company_id, return_ratio, movements = val_dataset[idx]\n",
    "    company_id = company_id.item()\n",
    "    industry_id = industry_id.item()\n",
    "    \n",
    "    val_company_sequences[company_id].append({\n",
    "        'features': features,\n",
    "        'industry_id': industry_id,\n",
    "        'return_ratio': return_ratio.item(),\n",
    "        'movements': movements.item(),\n",
    "        'idx': idx\n",
    "    })\n",
    "    sector_stock_map[industry_id].add(company_id)\n",
    "    stock_sector_map[company_id].add(industry_id)\n",
    "\n",
    "# Process validation embeddings\n",
    "val_attentive_embeddings = {}\n",
    "val_stock_returns_map = defaultdict(list)\n",
    "val_stock_movements_map = defaultdict(list)\n",
    "\n",
    "# Create attentive embeddings for validation data using TransformerSequentialLearner\n",
    "stsl_model = TransformerSequentialLearner(\n",
    "    input_size=val_dataset[0][0].shape[1],\n",
    "    hidden_size=HIDDEN_SIZE\n",
    ")\n",
    "stsl_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for company_id, sequences in val_company_sequences.items():\n",
    "        # Sort by original index to maintain temporal ordering\n",
    "        sequences.sort(key=lambda x: x['idx'])\n",
    "        \n",
    "        # Process all sequences for this company in order\n",
    "        company_attentive_embeddings = []\n",
    "        \n",
    "        for seq in sequences:\n",
    "            features = seq['features'].unsqueeze(0)\n",
    "            industry_id = seq['industry_id']\n",
    "            return_ratio = seq['return_ratio']\n",
    "            movements = seq['movements']\n",
    "            \n",
    "            # Store return and movement information\n",
    "            val_stock_returns_map[company_id].append(return_ratio)\n",
    "            val_stock_movements_map[company_id].append(movements)\n",
    "            \n",
    "            # Get embeddings using the sequential model\n",
    "            context, _ = stsl_model(features)\n",
    "            company_attentive_embeddings.append(context.squeeze(0).cpu().numpy())\n",
    "        \n",
    "        # Store the entire temporal sequence for this company\n",
    "        if company_attentive_embeddings:  # Only store if we have data\n",
    "            val_attentive_embeddings[company_id] = np.stack(company_attentive_embeddings, axis=0)\n",
    "print(f\"Test attentive embeddings processed: {len(val_attentive_embeddings)} companies\")\n",
    "\n",
    "# Add enhanced sector pooling function\n",
    "def enhanced_sector_pooling(graph_data):\n",
    "    x = graph_data.x\n",
    "    sector_id = graph_data.sector_id\n",
    "    \n",
    "\n",
    "    torch.manual_seed(sector_id * 100)\n",
    "\n",
    "    if sector_id % 4 == 0:  \n",
    " \n",
    "        combined_pool = 0.5 * torch.max(x, dim=0)[0] + 0.2 * torch.mean(x, dim=0) + 0.3 * torch.std(x, dim=0)\n",
    "    elif sector_id % 4 == 1:\n",
    "\n",
    "        combined_pool = 0.2 * torch.max(x, dim=0)[0] + 0.5 * torch.min(x, dim=0)[0] + 0.3 * torch.quantile(x, 0.25, dim=0)\n",
    "    elif sector_id % 4 == 2: \n",
    "\n",
    "        combined_pool = 0.3 * torch.mean(x, dim=0) + 0.7 * torch.std(x, dim=0)\n",
    "    else:  \n",
    "\n",
    "        combined_pool = 0.4 * torch.quantile(x, 0.5, dim=0) + 0.3 * torch.quantile(x, 0.75, dim=0) + 0.3 * torch.quantile(x, 0.25, dim=0)\n",
    "\n",
    "    sector_factor = ((sector_id % 7) + 1) / 4.0  \n",
    "    if sector_id % 3 == 0:\n",
    "\n",
    "        combined_pool = torch.tanh(sector_factor * combined_pool)\n",
    "    elif sector_id % 3 == 1:\n",
    "\n",
    "        combined_pool = F.leaky_relu(sector_factor * combined_pool, negative_slope=0.1)\n",
    "    else:\n",
    "\n",
    "        combined_pool = torch.clamp(torch.exp(sector_factor * combined_pool * 0.1) - 1, -5, 5)\n",
    "\n",
    "    dim = combined_pool.size(0)\n",
    "\n",
    "    noise_scaling = 0.5 + (sector_id % 10) * 0.1 \n",
    "    orthogonal_noise = torch.zeros_like(combined_pool)\n",
    "\n",
    "    for i in range(dim):\n",
    "        phase = (sector_id * 0.1) + (i * 0.3 * (1 + sector_id % 5))\n",
    "        orthogonal_noise[i] = torch.sin(torch.tensor(phase * math.pi))\n",
    "\n",
    "    noise = (torch.randn_like(combined_pool) * 0.4) + (orthogonal_noise * noise_scaling)\n",
    "    combined_pool = combined_pool + noise\n",
    "\n",
    "    if sector_id % 2 == 0:\n",
    "\n",
    "        norm = torch.norm(combined_pool)\n",
    "        if norm > 0:\n",
    "            combined_pool = combined_pool / norm\n",
    "    \n",
    "    return combined_pool\n",
    "# Function to evaluate on validation data\n",
    "def evaluate_on_validation(trained_model):\n",
    "    \"\"\"\n",
    "    Evaluate the TransformerFinGAT model on validation data by creating new embeddings from validation data\n",
    "    \"\"\"\n",
    "    print(\"Creating test embeddings from scratch...\")\n",
    "    global val_weekly_long_term_embeddings, val_weekly_inter_sector_embeddings\n",
    "    \n",
    "    val_weekly_intra_sector_graphs = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    # Find maximum weeks in validation data\n",
    "    max_val_weeks = max([len(emb) for emb in val_attentive_embeddings.values()]) if val_attentive_embeddings else 0\n",
    "    \n",
    "    # Process each timepoint (week) separately\n",
    "    for week_idx in range(max_val_weeks):\n",
    "        print(f\"Processing test week {week_idx}\")\n",
    "        \n",
    "        # Group by sector\n",
    "        for sector_id, sector_stocks in sector_stock_map.items():\n",
    "            sector_features = []\n",
    "            valid_indices = []\n",
    "            \n",
    "            # Collect stock embeddings in this sector-week\n",
    "            for stock_idx in sector_stocks:\n",
    "                if stock_idx in val_attentive_embeddings and week_idx < len(val_attentive_embeddings[stock_idx]):\n",
    "                    tensor_embedding = torch.tensor(val_attentive_embeddings[stock_idx][week_idx], dtype=torch.float32)\n",
    "                    sector_features.append(tensor_embedding)\n",
    "                    valid_indices.append(stock_idx)\n",
    "            \n",
    "            # Create graph if >= 2 stocks\n",
    "            if len(sector_features) >= 2:\n",
    "                edge_index = []\n",
    "                num_nodes = len(valid_indices)\n",
    "                \n",
    "                # Fully-connected edges without self-loops\n",
    "                for i in range(num_nodes):\n",
    "                    for j in range(num_nodes):\n",
    "                        if i != j:\n",
    "                            edge_index.append([i, j])\n",
    "                \n",
    "                # Store the graph with additional sector_id attribute\n",
    "                val_weekly_intra_sector_graphs[week_idx][sector_id] = Data(\n",
    "                    x=torch.stack(sector_features),\n",
    "                    edge_index=torch.tensor(edge_index).t().contiguous(),\n",
    "                    original_indices=valid_indices,\n",
    "                    sector_id=sector_id\n",
    "                )\n",
    "    \n",
    "    # STEP 2: Process validation graphs through IntraSectorGAT\n",
    "    val_sector_embeddings = []\n",
    "    gat_model = IntraSectorGAT(HIDDEN_SIZE=HIDDEN_SIZE)  # Reuse the model architecture\n",
    "    gat_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for week_idx in val_weekly_intra_sector_graphs:\n",
    "            for sector_id, graph in val_weekly_intra_sector_graphs[week_idx].items():\n",
    "                graph = graph\n",
    "                out = gat_model(graph)\n",
    "                val_sector_embeddings.append({\n",
    "                    'embeddings': out.cpu(),\n",
    "                    'original_indices': graph.original_indices,\n",
    "                    'sector_id': graph.sector_id,\n",
    "                    'week_idx': week_idx\n",
    "                })\n",
    "    \n",
    "    # STEP 3: Create pooled embeddings for sectors using enhanced pooling\n",
    "    val_weekly_sector_pooled_embeddings = defaultdict(dict)\n",
    "    \n",
    "    if val_sector_embeddings:\n",
    "        embedding_dim = val_sector_embeddings[0]['embeddings'].shape[1]\n",
    "        \n",
    "        for week_idx in sorted(val_weekly_intra_sector_graphs.keys()):\n",
    "            for sector_id in val_weekly_intra_sector_graphs[week_idx]:\n",
    "                graph = val_weekly_intra_sector_graphs[week_idx][sector_id]\n",
    "                pooled_embedding = enhanced_sector_pooling(graph)\n",
    "                val_weekly_sector_pooled_embeddings[week_idx][sector_id] = pooled_embedding\n",
    "    \n",
    "    # STEP 4: Create sector embeddings using the direct approach instead of InterSectorGAT\n",
    "    val_weekly_inter_sector_embeddings = defaultdict(dict)\n",
    "    \n",
    "    if val_sector_embeddings:\n",
    "        # Get number of sectors and create the embedding layer\n",
    "        num_sectors = len(sector_stock_map)\n",
    "        sector_embedding = nn.Embedding(num_sectors + 10, HIDDEN_SIZE)\n",
    "        \n",
    "        # Add a projection layer to get the desired output dimension\n",
    "        projection = nn.Sequential(\n",
    "            nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE * 2),\n",
    "            nn.LayerNorm(HIDDEN_SIZE * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_SIZE * 2, HIDDEN_SIZE)\n",
    "        )\n",
    "        \n",
    "        # Use the same set of weeks as in the pooled embeddings\n",
    "        for week_idx in sorted(val_weekly_sector_pooled_embeddings.keys()):\n",
    "            sectors_this_week = val_weekly_sector_pooled_embeddings[week_idx]\n",
    "            \n",
    "            # Process each sector\n",
    "            for sector_id in sectors_this_week.keys():\n",
    "                # Use direct sector ID for embedding lookup\n",
    "                sector_tensor = torch.tensor([sector_id])\n",
    "                \n",
    "                # Get the sector embedding and apply projection\n",
    "                with torch.no_grad():\n",
    "                    embedding = sector_embedding(sector_tensor)\n",
    "                    embedding = projection(embedding)\n",
    "                    \n",
    "                # Store the sector embedding for this week\n",
    "                val_weekly_inter_sector_embeddings[week_idx][sector_id] = embedding.squeeze(0).cpu()\n",
    "    \n",
    "    # STEP 5: Organize company sector embeddings\n",
    "    val_company_sector_embeddings = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    for sector_data in val_sector_embeddings:\n",
    "        week_idx = sector_data['week_idx']\n",
    "        original_indices = sector_data['original_indices']\n",
    "        embeddings = sector_data['embeddings']\n",
    "        \n",
    "        for i, company_idx in enumerate(original_indices):\n",
    "            val_company_sector_embeddings[company_idx][week_idx] = embeddings[i]\n",
    "    \n",
    "    # STEP 6: Process long-term embeddings using transformer learners\n",
    "    val_weekly_long_term_embeddings = defaultdict(lambda: defaultdict(dict))\n",
    "    SEQ_LENGTH = 4  # Same as training\n",
    "    \n",
    "    if val_sector_embeddings:\n",
    "        # Initialize long term transformer learners\n",
    "        long_term_g = LongTermTransformerLearner(\n",
    "            input_size=val_sector_embeddings[0]['embeddings'].shape[1],\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            lookback_weeks=SEQ_LENGTH\n",
    "        )\n",
    "        \n",
    "        long_term_a = LongTermTransformerLearner(\n",
    "            input_size=next(iter(val_attentive_embeddings.values())).shape[1],\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            lookback_weeks=SEQ_LENGTH\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get the maximum week index\n",
    "            if val_company_sector_embeddings:\n",
    "                max_weeks = max([max(weekly_data.keys()) for weekly_data in val_company_sector_embeddings.values()])\n",
    "                \n",
    "                # Process each week from SEQ_LENGTH onwards\n",
    "                for current_week in range(SEQ_LENGTH, max_weeks + 1):\n",
    "                    print(f\"Processing validation long-term embeddings for week {current_week}\")\n",
    "                    # Process each company for this week\n",
    "                    for company_idx, weekly_data in val_company_sector_embeddings.items():\n",
    "                        # Check if we have data for this company in this week\n",
    "                        if current_week not in weekly_data:\n",
    "                            continue\n",
    "                            \n",
    "                        # Check if we have enough history\n",
    "                        history_weeks = [w for w in range(current_week - SEQ_LENGTH, current_week)]\n",
    "                        if not all(w in weekly_data for w in history_weeks):\n",
    "                            continue\n",
    "                            \n",
    "                        # Gather the sliding window of embeddings\n",
    "                        graph_window = []\n",
    "                        for w in history_weeks:\n",
    "                            if w in weekly_data:\n",
    "                                graph_emb = torch.tensor(weekly_data[w]).unsqueeze(0)\n",
    "                                graph_window.append(graph_emb)\n",
    "                        \n",
    "                        # Process attentive embeddings if available\n",
    "                        if company_idx in val_attentive_embeddings:\n",
    "                            attentive_seqs = val_attentive_embeddings[company_idx]\n",
    "                            \n",
    "                            if len(attentive_seqs) >= SEQ_LENGTH:\n",
    "                                attentive_window = []\n",
    "                                for i in range(SEQ_LENGTH):\n",
    "                                    seq_idx = len(attentive_seqs) - SEQ_LENGTH + i\n",
    "                                    if seq_idx >= 0 and seq_idx < len(attentive_seqs):\n",
    "                                        att_emb = torch.tensor(attentive_seqs[seq_idx], dtype=torch.float32).unsqueeze(0)\n",
    "                                        attentive_window.append(att_emb)\n",
    "                                \n",
    "                                if len(graph_window) == SEQ_LENGTH and len(attentive_window) == SEQ_LENGTH:\n",
    "                                    tau_G = long_term_g(graph_window)\n",
    "                                    tau_A = long_term_a(attentive_window)\n",
    "                                    \n",
    "                                    # Store the results\n",
    "                                    val_weekly_long_term_embeddings[current_week][company_idx] = {\n",
    "                                        'graph': tau_G.cpu().numpy(),\n",
    "                                        'attentive': tau_A.cpu().numpy()\n",
    "                                    }\n",
    "    \n",
    "    # STEP 7: Create final evaluation data using the latest week\n",
    "    val_data = {\n",
    "        'attentive_embs': [],\n",
    "        'graph_embs': [],\n",
    "        'sector_embs': [],\n",
    "        'returns': [],\n",
    "        'movements': [],\n",
    "        'companies': []\n",
    "    }\n",
    "    \n",
    "    # Use the last week for prediction\n",
    "    if val_weekly_long_term_embeddings:\n",
    "        last_week = max(val_weekly_long_term_embeddings.keys())\n",
    "        print(f\"Using week {last_week} for final evaluation\")\n",
    "        \n",
    "        for company_idx, company_data in val_weekly_long_term_embeddings[last_week].items():\n",
    "            # Get the stock's sector\n",
    "            if company_idx not in stock_sector_map or len(stock_sector_map[company_idx]) == 0:\n",
    "                continue\n",
    "                \n",
    "            sector_id = list(stock_sector_map[company_idx])[0]\n",
    "            \n",
    "            # Check if we have the sector embedding for this week\n",
    "            if sector_id not in val_weekly_inter_sector_embeddings[last_week]:\n",
    "                continue\n",
    "                \n",
    "            # Extract embeddings\n",
    "            attentive_emb = torch.tensor(company_data['attentive'], dtype=torch.float32)\n",
    "            graph_emb = torch.tensor(company_data['graph'], dtype=torch.float32)\n",
    "            sector_emb = val_weekly_inter_sector_embeddings[last_week][sector_id]\n",
    "            \n",
    "            # Get return and movement labels\n",
    "            if company_idx in val_stock_returns_map and val_stock_returns_map[company_idx]:\n",
    "                return_ratio = val_stock_returns_map[company_idx][0]\n",
    "                movement = 1.0 if return_ratio > 0 else 0.0\n",
    "                \n",
    "                # Add to evaluation data\n",
    "                val_data['attentive_embs'].append(attentive_emb)\n",
    "                val_data['graph_embs'].append(graph_emb)\n",
    "                val_data['sector_embs'].append(sector_emb)\n",
    "                val_data['companies'].append(company_idx)\n",
    "                val_data['returns'].append(return_ratio)\n",
    "                val_data['movements'].append(movement)\n",
    "    \n",
    "    # Perform evaluation if we have data\n",
    "    if val_data['returns']:\n",
    "        val_data['attentive_embs'] = torch.stack(val_data['attentive_embs'])\n",
    "        val_data['graph_embs'] = torch.stack(val_data['graph_embs'])\n",
    "        val_data['sector_embs'] = torch.stack(val_data['sector_embs'])\n",
    "        val_data['returns'] = torch.tensor(val_data['returns'], dtype=torch.float32)\n",
    "        val_data['movements'] = torch.tensor(val_data['movements'], dtype=torch.float32)\n",
    "        \n",
    "        # Evaluate\n",
    "        trained_model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Move to device\n",
    "            attentive_embs = val_data['attentive_embs']\n",
    "            graph_embs = val_data['graph_embs']\n",
    "            sector_embs = val_data['sector_embs']\n",
    "            \n",
    "            # Get predictions - using the transformer model\n",
    "            return_preds, movement_preds = trained_model(attentive_embs, graph_embs, sector_embs)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            return_preds_np = return_preds.cpu().numpy()\n",
    "            return_targets_np = val_data['returns'].numpy()\n",
    "            movement_preds_np = movement_preds.cpu().numpy()\n",
    "            movement_targets_np = val_data['movements'].numpy()\n",
    "            \n",
    "            # Correlation for returns\n",
    "            from scipy.stats import spearmanr\n",
    "            corr, _ = spearmanr(return_preds_np, return_targets_np)\n",
    "            \n",
    "            # Accuracy for movement prediction\n",
    "            threshold = 0.5\n",
    "            binary_preds = [1 if p > threshold else 0 for p in movement_preds_np]\n",
    "            accuracy = sum(p == t for p, t in zip(binary_preds, movement_targets_np)) / len(binary_preds)\n",
    "            \n",
    "\n",
    "            \n",
    "            # Calculate MRR metrics\n",
    "            pred_target_pairs = list(zip(return_preds_np, return_targets_np, val_data['companies']))\n",
    "            pred_ranking = sorted(pred_target_pairs, key=lambda x: x[0], reverse=True)\n",
    "            company_to_pred_rank = {company: i+1 for i, (_, _, company) in enumerate(pred_ranking)}\n",
    "            \n",
    "            true_ranking = sorted(\n",
    "                [(target, company) for (_, target, company) in pred_target_pairs],\n",
    "                key=lambda x: x[0], \n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            mrr_values = {}\n",
    "            top_k_values = [5, 10, 20, 30, 100]\n",
    "            \n",
    "            for k in top_k_values:\n",
    "                mrr_at_k = []\n",
    "                for j, (_, company) in enumerate(true_ranking[:min(k, len(true_ranking))]):\n",
    "                    if company in company_to_pred_rank:\n",
    "                        mrr_at_k.append(1.0 / company_to_pred_rank[company])\n",
    "                \n",
    "                mrr_values[k] = sum(mrr_at_k) / len(mrr_at_k) if mrr_at_k else 0\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"\\nValidation Metrics (using transformer model):\")\n",
    "            print(f\"Number of validation samples: {len(return_preds_np)}\")\n",
    "            print(f\"Return prediction correlation: {corr:.4f}\")\n",
    "            print(f\"Movement prediction accuracy: {accuracy:.4f}\")\n",
    "\n",
    "            \n",
    "            print(\"\\nMean Reciprocal Rank (MRR) Metrics:\")\n",
    "            for k in top_k_values:\n",
    "                print(f\"MRR@{k}: {mrr_values[k]:.4f}\")\n",
    "            \n",
    "            # Calculate IRR metrics\n",
    "            irr_values = {}\n",
    "            \n",
    "            for k in top_k_values:\n",
    "                # Sort predictions and get top and bottom k stocks\n",
    "                pred_sorted_indices = np.argsort(return_preds_np)[::-1]\n",
    "                \n",
    "                # Get actual returns for top k and bottom k predicted stocks\n",
    "                if len(pred_sorted_indices) >= k*2:\n",
    "                    top_k_returns = [return_targets_np[i] for i in pred_sorted_indices[:k]]\n",
    "                    bottom_k_returns = [return_targets_np[i] for i in pred_sorted_indices[-k:]]\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    top_k_mean = np.mean(top_k_returns)\n",
    "                    bottom_k_mean = np.mean(bottom_k_returns)\n",
    "                    excess_return = top_k_mean - bottom_k_mean\n",
    "                    \n",
    "                    # Calculate tracking error as standard deviation of return differences\n",
    "                    if len(top_k_returns) > 1:\n",
    "                        tracking_error = np.std(np.array(top_k_returns) - np.array(bottom_k_returns))\n",
    "                        ir = excess_return / tracking_error if tracking_error > 0 else 0\n",
    "                    else:\n",
    "                        ir = 0\n",
    "                        \n",
    "                    irr_values[k] = {\n",
    "                        'ir': ir,\n",
    "                        'top_returns': top_k_mean,\n",
    "                        'bottom_returns': bottom_k_mean,\n",
    "                        'excess_return': excess_return\n",
    "                    }\n",
    "            \n",
    "            # Print IRR metrics\n",
    "            print(\"\\nInformation Ratio (IRR) Metrics:\")\n",
    "            for k in top_k_values:\n",
    "                if k in irr_values:\n",
    "                    print(f\"IRR@{k}: {irr_values[k]['ir']:.4f} (Excess Return: {irr_values[k]['excess_return']:.4f}, \"\n",
    "                          f\"Top-{k} Avg: {irr_values[k]['top_returns']:.4f}, Bottom-{k} Avg: {irr_values[k]['bottom_returns']:.4f})\")\n",
    "                    \n",
    "            mae = np.mean(np.abs(return_preds_np - return_targets_np))\n",
    "            print(f\"Mean Absolute Error (MAE): {mae:.6f}\")\n",
    "\n",
    "\n",
    "            return corr, accuracy, mae, mrr_values\n",
    "    else:\n",
    "        print(\"No validation data available for evaluation\")\n",
    "        return None, None, None, None\n",
    "\n",
    "print(\"\\n--- Evaluating on Test Data with Fresh Embeddings ---\")\n",
    "test_metrics = evaluate_on_validation(model)  \n",
    "print(\"\\n--- Evaluation Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ef13750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_company_prediction_csvs(model):\n",
    "    print(\"Generating per-company prediction CSV files...\")\n",
    "    \n",
    "    # Create a directory to store company CSVs\n",
    "    output_dir = 'company_predictions'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    # Get company names directly from the test dataset\n",
    "    company_id_to_name = {idx: name for name, idx in val_dataset.company_map.items()}\n",
    "    \n",
    "    # Verify we have names\n",
    "    print(f\"Found {len(company_id_to_name)} company name mappings from dataset\")\n",
    "    \n",
    "    predictions_data = {\n",
    "        'date': [],\n",
    "        'company_id': [],\n",
    "        'company_name': [],\n",
    "        'actual_return': [],\n",
    "        'predicted_return': [],\n",
    "        'movement_actual': [],\n",
    "        'movement_predicted': [],\n",
    "        'week_idx': []  # Add week index for filtering later\n",
    "    }\n",
    "    \n",
    "    if val_weekly_long_term_embeddings:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for week_idx in sorted(val_weekly_long_term_embeddings.keys()):\n",
    "                try:\n",
    "                    dates = df_val_date.index.unique()\n",
    "                    if week_idx < len(dates):\n",
    "                        current_date = dates[week_idx]\n",
    "                    else:\n",
    "                        current_date = pd.Timestamp(end_date)\n",
    "                except:\n",
    "                    current_date = pd.Timestamp(start_date) + pd.Timedelta(days=7*week_idx)\n",
    "                \n",
    "                for company_idx, company_data in val_weekly_long_term_embeddings[week_idx].items():\n",
    "                    if (company_idx not in stock_sector_map or \n",
    "                        len(stock_sector_map[company_idx]) == 0):\n",
    "                        continue\n",
    "                    \n",
    "                    sector_id = list(stock_sector_map[company_idx])[0]\n",
    "                    if sector_id not in val_weekly_inter_sector_embeddings[week_idx]:\n",
    "                        continue\n",
    "                    \n",
    "                    attentive_emb = torch.tensor(company_data['attentive'], dtype=torch.float32).unsqueeze(0)\n",
    "                    graph_emb = torch.tensor(company_data['graph'], dtype=torch.float32).unsqueeze(0)\n",
    "                    sector_emb = val_weekly_inter_sector_embeddings[week_idx][sector_id].unsqueeze(0)\n",
    "                    \n",
    "                    if company_idx in val_stock_returns_map and len(val_stock_returns_map[company_idx]) > week_idx:\n",
    "                        actual_return = val_stock_returns_map[company_idx][week_idx]\n",
    "                        actual_movement = 1.0 if actual_return > 0 else 0.0\n",
    "                        \n",
    "                        return_pred, movement_pred = model(attentive_emb, graph_emb, sector_emb)\n",
    "                        \n",
    "                        # Get real company name from mapping\n",
    "                        company_name = company_id_to_name.get(company_idx, f\"Company_{company_idx}\")\n",
    "                        \n",
    "                        predictions_data['date'].append(current_date)\n",
    "                        predictions_data['company_id'].append(company_idx)\n",
    "                        predictions_data['company_name'].append(company_name)\n",
    "                        predictions_data['actual_return'].append(actual_return)\n",
    "                        predictions_data['predicted_return'].append(return_pred.item())\n",
    "                        predictions_data['movement_actual'].append(actual_movement)\n",
    "                        predictions_data['movement_predicted'].append(movement_pred.item())\n",
    "                        predictions_data['week_idx'].append(week_idx)\n",
    "    \n",
    "    if predictions_data['date']:\n",
    "        # Create the main predictions DataFrame\n",
    "        all_preds_df = pd.DataFrame(predictions_data)\n",
    "        \n",
    "        # Convert company_id to string for better readability\n",
    "        all_preds_df['company_id'] = all_preds_df['company_id'].astype(str)\n",
    "        \n",
    "        # Save a summary file with all predictions\n",
    "        all_preds_df.to_csv(os.path.join(output_dir, 'all_predictions.csv'), index=False)\n",
    "        \n",
    "        # Group by company and save individual CSVs\n",
    "        companies = all_preds_df['company_id'].unique()\n",
    "        print(f\"Creating CSV files for {len(companies)} companies...\")\n",
    "        \n",
    "        for company_id in companies:\n",
    "            company_df = all_preds_df[all_preds_df['company_id'] == company_id]\n",
    "            company_name = company_df['company_name'].iloc[0]\n",
    "            \n",
    "            # Create a safe filename (remove special characters)\n",
    "            safe_name = ''.join(c if c.isalnum() else '_' for c in str(company_name))\n",
    "            filename = f\"{company_id}_{safe_name}.csv\"\n",
    "            \n",
    "            # Sort by date\n",
    "            company_df = company_df.sort_values('date')\n",
    "            \n",
    "            # Save to CSV\n",
    "            company_df.to_csv(os.path.join(output_dir, filename), index=False)\n",
    "        \n",
    "        print(f\"Successfully created {len(companies)} company-specific CSV files in '{output_dir}' directory\")\n",
    "        \n",
    "        # Print top companies from the last week\n",
    "        last_week = all_preds_df['week_idx'].max()\n",
    "        last_week_preds = all_preds_df[all_preds_df['week_idx'] == last_week]\n",
    "        \n",
    "        # Top 5 by predicted return\n",
    "        top5_predicted = last_week_preds.sort_values('predicted_return', ascending=False).head(5)\n",
    "        print(\"\\nTop 5 Companies by Predicted Return (Last Week):\")\n",
    "        for idx, row in top5_predicted.iterrows():\n",
    "            print(f\"{row['company_name']} - Predicted: {row['predicted_return']:.4f}, Actual: {row['actual_return']:.4f}\")\n",
    "        \n",
    "        # Top 5 by actual return\n",
    "        top5_actual = last_week_preds.sort_values('actual_return', ascending=False).head(5)\n",
    "        print(\"\\nTop 5 Companies by Actual Return (Last Week):\")\n",
    "        for idx, row in top5_actual.iterrows():\n",
    "            print(f\"{row['company_name']} - Actual: {row['actual_return']:.4f}, Predicted: {row['predicted_return']:.4f}\")\n",
    "        \n",
    "        return all_preds_df\n",
    "    else:\n",
    "        print(\"No prediction data available to save\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4d68f2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating per-company prediction CSV files...\n",
      "Found 445 company name mappings from dataset\n",
      "Creating CSV files for 445 companies...\n",
      "Successfully created 445 company-specific CSV files in 'company_predictions' directory\n",
      "\n",
      "Top 5 Companies by Predicted Return (Last Week):\n",
      "TANLA - Predicted: -0.3067, Actual: -0.0041\n",
      "BSOFT - Predicted: -0.3081, Actual: -0.0114\n",
      "TATAELXSI - Predicted: -0.3084, Actual: 0.0159\n",
      "LTTS - Predicted: -0.3086, Actual: 0.0034\n",
      "CYIENT - Predicted: -0.3096, Actual: 0.0261\n",
      "\n",
      "Top 5 Companies by Actual Return (Last Week):\n",
      "DBREALTY - Actual: 0.1938, Predicted: -0.3889\n",
      "MRPL - Actual: 0.1497, Predicted: -0.3758\n",
      "APLLTD - Actual: 0.1412, Predicted: -0.3183\n",
      "RKFORGE - Actual: 0.1324, Predicted: -0.4108\n",
      "TEJASNET - Actual: 0.1258, Predicted: -0.4046\n",
      "\n",
      "Total predictions generated: 17355\n",
      "Number of unique companies: 445\n",
      "Date range: 2025-01-17 00:00:00+05:30 to 2025-03-12 00:00:00+05:30\n"
     ]
    }
   ],
   "source": [
    "company_predictions = generate_company_prediction_csvs(model)\n",
    "\n",
    "if company_predictions is not None:\n",
    "    print(f\"\\nTotal predictions generated: {len(company_predictions)}\")\n",
    "    print(f\"Number of unique companies: {company_predictions['company_id'].nunique()}\")\n",
    "    print(f\"Date range: {company_predictions['date'].min()} to {company_predictions['date'].max()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Universal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
