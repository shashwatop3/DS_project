{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b11cbce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Subset\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bc7c8a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import HierarchicalStockDataset, TransformerSequentialLearner, IntraSectorGAT, LongTermTransformerLearner, EmbeddingFusion, FinGAT, MultiTaskLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8716ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aa4b33bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'fingat_tranformer_model.pth'\n",
    "\n",
    "\n",
    "model = FinGAT(\n",
    "    attentive_dim=HIDDEN_SIZE,\n",
    "    graph_dim=HIDDEN_SIZE,\n",
    "    sector_dim=HIDDEN_SIZE\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f949326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 10680)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('stock_data/processed/merged_stock_data_with_enhanced_features.parquet')\n",
    "df.head()\n",
    "\n",
    "df.index = pd.to_datetime(df.index)  \n",
    "\n",
    "start_date = \"2025-01-01\"\n",
    "end_date = \"2025-03-10\"\n",
    "\n",
    "\n",
    "df_val_date  = df.loc[start_date:end_date]\n",
    "print(df_val_date.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c9be3b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset contains 19135 samples\n",
      "Validation attentive embeddings processed: 445 companies\n",
      "\n",
      "--- Evaluating on Validation Data with Fresh Embeddings ---\n",
      "Creating validation embeddings from scratch...\n",
      "Processing validation week 0\n",
      "Processing validation week 1\n",
      "Processing validation week 2\n",
      "Processing validation week 3\n",
      "Processing validation week 4\n",
      "Processing validation week 5\n",
      "Processing validation week 6\n",
      "Processing validation week 7\n",
      "Processing validation week 8\n",
      "Processing validation week 9\n",
      "Processing validation week 10\n",
      "Processing validation week 11\n",
      "Processing validation week 12\n",
      "Processing validation week 13\n",
      "Processing validation week 14\n",
      "Processing validation week 15\n",
      "Processing validation week 16\n",
      "Processing validation week 17\n",
      "Processing validation week 18\n",
      "Processing validation week 19\n",
      "Processing validation week 20\n",
      "Processing validation week 21\n",
      "Processing validation week 22\n",
      "Processing validation week 23\n",
      "Processing validation week 24\n",
      "Processing validation week 25\n",
      "Processing validation week 26\n",
      "Processing validation week 27\n",
      "Processing validation week 28\n",
      "Processing validation week 29\n",
      "Processing validation week 30\n",
      "Processing validation week 31\n",
      "Processing validation week 32\n",
      "Processing validation week 33\n",
      "Processing validation week 34\n",
      "Processing validation week 35\n",
      "Processing validation week 36\n",
      "Processing validation week 37\n",
      "Processing validation week 38\n",
      "Processing validation week 39\n",
      "Processing validation week 40\n",
      "Processing validation week 41\n",
      "Processing validation week 42\n",
      "Creating new GATConv model for input dimension 16\n",
      "Processing validation long-term embeddings for week 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gn/mtbdxhjn1697bp_qhyd6y0cr0000gn/T/ipykernel_84453/734140667.py:278: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  graph_emb = torch.tensor(weekly_data[w]).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation long-term embeddings for week 5\n",
      "Processing validation long-term embeddings for week 6\n",
      "Processing validation long-term embeddings for week 7\n",
      "Processing validation long-term embeddings for week 8\n",
      "Processing validation long-term embeddings for week 9\n",
      "Processing validation long-term embeddings for week 10\n",
      "Processing validation long-term embeddings for week 11\n",
      "Processing validation long-term embeddings for week 12\n",
      "Processing validation long-term embeddings for week 13\n",
      "Processing validation long-term embeddings for week 14\n",
      "Processing validation long-term embeddings for week 15\n",
      "Processing validation long-term embeddings for week 16\n",
      "Processing validation long-term embeddings for week 17\n",
      "Processing validation long-term embeddings for week 18\n",
      "Processing validation long-term embeddings for week 19\n",
      "Processing validation long-term embeddings for week 20\n",
      "Processing validation long-term embeddings for week 21\n",
      "Processing validation long-term embeddings for week 22\n",
      "Processing validation long-term embeddings for week 23\n",
      "Processing validation long-term embeddings for week 24\n",
      "Processing validation long-term embeddings for week 25\n",
      "Processing validation long-term embeddings for week 26\n",
      "Processing validation long-term embeddings for week 27\n",
      "Processing validation long-term embeddings for week 28\n",
      "Processing validation long-term embeddings for week 29\n",
      "Processing validation long-term embeddings for week 30\n",
      "Processing validation long-term embeddings for week 31\n",
      "Processing validation long-term embeddings for week 32\n",
      "Processing validation long-term embeddings for week 33\n",
      "Processing validation long-term embeddings for week 34\n",
      "Processing validation long-term embeddings for week 35\n",
      "Processing validation long-term embeddings for week 36\n",
      "Processing validation long-term embeddings for week 37\n",
      "Processing validation long-term embeddings for week 38\n",
      "Processing validation long-term embeddings for week 39\n",
      "Processing validation long-term embeddings for week 40\n",
      "Processing validation long-term embeddings for week 41\n",
      "Processing validation long-term embeddings for week 42\n",
      "Using week 42 for final evaluation\n",
      "\n",
      "Validation Metrics (using transformer model):\n",
      "Number of validation samples: 445\n",
      "Return prediction correlation: 0.0149\n",
      "Movement prediction accuracy: 0.4562\n",
      "\n",
      "Mean Reciprocal Rank (MRR) Metrics:\n",
      "MRR@5: 0.0094\n",
      "MRR@10: 0.0071\n",
      "MRR@20: 0.0102\n",
      "MRR@30: 0.0086\n",
      "MRR@100: 0.0123\n",
      "\n",
      "Information Ratio (IRR) Metrics:\n",
      "IRR@5: 0.3173 (Excess Return: 0.0048, Top-5 Avg: -0.0100, Bottom-5 Avg: -0.0148)\n",
      "IRR@10: 0.7158 (Excess Return: 0.0104, Top-10 Avg: -0.0074, Bottom-10 Avg: -0.0178)\n",
      "IRR@20: 0.0333 (Excess Return: 0.0007, Top-20 Avg: -0.0104, Bottom-20 Avg: -0.0111)\n",
      "IRR@30: 0.2063 (Excess Return: 0.0049, Top-30 Avg: -0.0072, Bottom-30 Avg: -0.0121)\n",
      "IRR@100: 0.0438 (Excess Return: 0.0012, Top-100 Avg: -0.0081, Bottom-100 Avg: -0.0092)\n",
      "\n",
      "--- Evaluation Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Create the validation dataset\n",
    "val_dataset = HierarchicalStockDataset(df_val_date)\n",
    "print(f\"Validation dataset contains {val_dataset.__len__()} samples\")\n",
    "\n",
    "# Process validation data similarly to training data\n",
    "val_company_sequences = defaultdict(list)\n",
    "sector_stock_map = defaultdict(set)\n",
    "stock_sector_map = defaultdict(set)\n",
    "\n",
    "# Organize validation samples by company\n",
    "for idx in range(len(val_dataset)):\n",
    "    features, industry_id, company_id, return_ratio, movements = val_dataset[idx]\n",
    "    company_id = company_id.item()\n",
    "    industry_id = industry_id.item()\n",
    "    \n",
    "    val_company_sequences[company_id].append({\n",
    "        'features': features,\n",
    "        'industry_id': industry_id,\n",
    "        'return_ratio': return_ratio.item(),\n",
    "        'movements': movements.item(),\n",
    "        'idx': idx\n",
    "    })\n",
    "    sector_stock_map[industry_id].add(company_id)\n",
    "    stock_sector_map[company_id].add(industry_id)\n",
    "\n",
    "# Process validation embeddings\n",
    "val_attentive_embeddings = {}\n",
    "val_stock_returns_map = defaultdict(list)\n",
    "val_stock_movements_map = defaultdict(list)\n",
    "\n",
    "# Create attentive embeddings for validation data using TransformerSequentialLearner\n",
    "stsl_model = TransformerSequentialLearner(\n",
    "    input_size=val_dataset[0][0].shape[1],\n",
    "    hidden_size=HIDDEN_SIZE\n",
    ")\n",
    "stsl_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for company_id, sequences in val_company_sequences.items():\n",
    "        # Sort by original index to maintain temporal ordering\n",
    "        sequences.sort(key=lambda x: x['idx'])\n",
    "        \n",
    "        # Process all sequences for this company in order\n",
    "        company_attentive_embeddings = []\n",
    "        \n",
    "        for seq in sequences:\n",
    "            features = seq['features'].unsqueeze(0)\n",
    "            industry_id = seq['industry_id']\n",
    "            return_ratio = seq['return_ratio']\n",
    "            movements = seq['movements']\n",
    "            \n",
    "            # Store return and movement information\n",
    "            val_stock_returns_map[company_id].append(return_ratio)\n",
    "            val_stock_movements_map[company_id].append(movements)\n",
    "            \n",
    "            # Get embeddings using the sequential model\n",
    "            context, _ = stsl_model(features)\n",
    "            company_attentive_embeddings.append(context.squeeze(0).cpu().numpy())\n",
    "        \n",
    "        # Store the entire temporal sequence for this company\n",
    "        if company_attentive_embeddings:  # Only store if we have data\n",
    "            val_attentive_embeddings[company_id] = np.stack(company_attentive_embeddings, axis=0)\n",
    "print(f\"Validation attentive embeddings processed: {len(val_attentive_embeddings)} companies\")\n",
    "\n",
    "# Add enhanced sector pooling function\n",
    "def enhanced_sector_pooling(graph_data):\n",
    "    x = graph_data.x\n",
    "    sector_id = graph_data.sector_id\n",
    "    \n",
    "\n",
    "    torch.manual_seed(sector_id * 100)\n",
    "\n",
    "    if sector_id % 4 == 0:  \n",
    " \n",
    "        combined_pool = 0.5 * torch.max(x, dim=0)[0] + 0.2 * torch.mean(x, dim=0) + 0.3 * torch.std(x, dim=0)\n",
    "    elif sector_id % 4 == 1:\n",
    "\n",
    "        combined_pool = 0.2 * torch.max(x, dim=0)[0] + 0.5 * torch.min(x, dim=0)[0] + 0.3 * torch.quantile(x, 0.25, dim=0)\n",
    "    elif sector_id % 4 == 2: \n",
    "\n",
    "        combined_pool = 0.3 * torch.mean(x, dim=0) + 0.7 * torch.std(x, dim=0)\n",
    "    else:  \n",
    "\n",
    "        combined_pool = 0.4 * torch.quantile(x, 0.5, dim=0) + 0.3 * torch.quantile(x, 0.75, dim=0) + 0.3 * torch.quantile(x, 0.25, dim=0)\n",
    "\n",
    "    sector_factor = ((sector_id % 7) + 1) / 4.0  \n",
    "    if sector_id % 3 == 0:\n",
    "\n",
    "        combined_pool = torch.tanh(sector_factor * combined_pool)\n",
    "    elif sector_id % 3 == 1:\n",
    "\n",
    "        combined_pool = F.leaky_relu(sector_factor * combined_pool, negative_slope=0.1)\n",
    "    else:\n",
    "\n",
    "        combined_pool = torch.clamp(torch.exp(sector_factor * combined_pool * 0.1) - 1, -5, 5)\n",
    "\n",
    "    dim = combined_pool.size(0)\n",
    "\n",
    "    noise_scaling = 0.5 + (sector_id % 10) * 0.1 \n",
    "    orthogonal_noise = torch.zeros_like(combined_pool)\n",
    "\n",
    "    for i in range(dim):\n",
    "        phase = (sector_id * 0.1) + (i * 0.3 * (1 + sector_id % 5))\n",
    "        orthogonal_noise[i] = torch.sin(torch.tensor(phase * math.pi))\n",
    "\n",
    "    noise = (torch.randn_like(combined_pool) * 0.4) + (orthogonal_noise * noise_scaling)\n",
    "    combined_pool = combined_pool + noise\n",
    "\n",
    "    if sector_id % 2 == 0:\n",
    "\n",
    "        norm = torch.norm(combined_pool)\n",
    "        if norm > 0:\n",
    "            combined_pool = combined_pool / norm\n",
    "    \n",
    "    return combined_pool\n",
    "# Function to evaluate on validation data\n",
    "def evaluate_on_validation(trained_model):\n",
    "    \"\"\"\n",
    "    Evaluate the TransformerFinGAT model on validation data by creating new embeddings from validation data\n",
    "    \"\"\"\n",
    "    print(\"Creating validation embeddings from scratch...\")\n",
    "    global val_weekly_long_term_embeddings, val_weekly_inter_sector_embeddings\n",
    "    \n",
    "    val_weekly_intra_sector_graphs = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    # Find maximum weeks in validation data\n",
    "    max_val_weeks = max([len(emb) for emb in val_attentive_embeddings.values()]) if val_attentive_embeddings else 0\n",
    "    \n",
    "    # Process each timepoint (week) separately\n",
    "    for week_idx in range(max_val_weeks):\n",
    "        print(f\"Processing validation week {week_idx}\")\n",
    "        \n",
    "        # Group by sector\n",
    "        for sector_id, sector_stocks in sector_stock_map.items():\n",
    "            sector_features = []\n",
    "            valid_indices = []\n",
    "            \n",
    "            # Collect stock embeddings in this sector-week\n",
    "            for stock_idx in sector_stocks:\n",
    "                if stock_idx in val_attentive_embeddings and week_idx < len(val_attentive_embeddings[stock_idx]):\n",
    "                    tensor_embedding = torch.tensor(val_attentive_embeddings[stock_idx][week_idx], dtype=torch.float32)\n",
    "                    sector_features.append(tensor_embedding)\n",
    "                    valid_indices.append(stock_idx)\n",
    "            \n",
    "            # Create graph if >= 2 stocks\n",
    "            if len(sector_features) >= 2:\n",
    "                edge_index = []\n",
    "                num_nodes = len(valid_indices)\n",
    "                \n",
    "                # Fully-connected edges without self-loops\n",
    "                for i in range(num_nodes):\n",
    "                    for j in range(num_nodes):\n",
    "                        if i != j:\n",
    "                            edge_index.append([i, j])\n",
    "                \n",
    "                # Store the graph with additional sector_id attribute\n",
    "                val_weekly_intra_sector_graphs[week_idx][sector_id] = Data(\n",
    "                    x=torch.stack(sector_features),\n",
    "                    edge_index=torch.tensor(edge_index).t().contiguous(),\n",
    "                    original_indices=valid_indices,\n",
    "                    sector_id=sector_id\n",
    "                )\n",
    "    \n",
    "    # STEP 2: Process validation graphs through IntraSectorGAT\n",
    "    val_sector_embeddings = []\n",
    "    gat_model = IntraSectorGAT(HIDDEN_SIZE=HIDDEN_SIZE)  # Reuse the model architecture\n",
    "    gat_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for week_idx in val_weekly_intra_sector_graphs:\n",
    "            for sector_id, graph in val_weekly_intra_sector_graphs[week_idx].items():\n",
    "                graph = graph\n",
    "                out = gat_model(graph)\n",
    "                val_sector_embeddings.append({\n",
    "                    'embeddings': out.cpu(),\n",
    "                    'original_indices': graph.original_indices,\n",
    "                    'sector_id': graph.sector_id,\n",
    "                    'week_idx': week_idx\n",
    "                })\n",
    "    \n",
    "    # STEP 3: Create pooled embeddings for sectors using enhanced pooling\n",
    "    val_weekly_sector_pooled_embeddings = defaultdict(dict)\n",
    "    \n",
    "    if val_sector_embeddings:\n",
    "        embedding_dim = val_sector_embeddings[0]['embeddings'].shape[1]\n",
    "        \n",
    "        for week_idx in sorted(val_weekly_intra_sector_graphs.keys()):\n",
    "            for sector_id in val_weekly_intra_sector_graphs[week_idx]:\n",
    "                graph = val_weekly_intra_sector_graphs[week_idx][sector_id]\n",
    "                pooled_embedding = enhanced_sector_pooling(graph)\n",
    "                val_weekly_sector_pooled_embeddings[week_idx][sector_id] = pooled_embedding\n",
    "    \n",
    "    # STEP 4: Create sector embeddings using the direct approach instead of InterSectorGAT\n",
    "    val_weekly_inter_sector_embeddings = defaultdict(dict)\n",
    "    \n",
    "    if val_sector_embeddings:\n",
    "        # Get number of sectors and create the embedding layer\n",
    "        num_sectors = len(sector_stock_map)\n",
    "        sector_embedding = nn.Embedding(num_sectors + 10, HIDDEN_SIZE)\n",
    "        \n",
    "        # Add a projection layer to get the desired output dimension\n",
    "        projection = nn.Sequential(\n",
    "            nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE * 2),\n",
    "            nn.LayerNorm(HIDDEN_SIZE * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_SIZE * 2, HIDDEN_SIZE)\n",
    "        )\n",
    "        \n",
    "        # Use the same set of weeks as in the pooled embeddings\n",
    "        for week_idx in sorted(val_weekly_sector_pooled_embeddings.keys()):\n",
    "            sectors_this_week = val_weekly_sector_pooled_embeddings[week_idx]\n",
    "            \n",
    "            # Process each sector\n",
    "            for sector_id in sectors_this_week.keys():\n",
    "                # Use direct sector ID for embedding lookup\n",
    "                sector_tensor = torch.tensor([sector_id])\n",
    "                \n",
    "                # Get the sector embedding and apply projection\n",
    "                with torch.no_grad():\n",
    "                    embedding = sector_embedding(sector_tensor)\n",
    "                    embedding = projection(embedding)\n",
    "                    \n",
    "                # Store the sector embedding for this week\n",
    "                val_weekly_inter_sector_embeddings[week_idx][sector_id] = embedding.squeeze(0).cpu()\n",
    "    \n",
    "    # STEP 5: Organize company sector embeddings\n",
    "    val_company_sector_embeddings = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    for sector_data in val_sector_embeddings:\n",
    "        week_idx = sector_data['week_idx']\n",
    "        original_indices = sector_data['original_indices']\n",
    "        embeddings = sector_data['embeddings']\n",
    "        \n",
    "        for i, company_idx in enumerate(original_indices):\n",
    "            val_company_sector_embeddings[company_idx][week_idx] = embeddings[i]\n",
    "    \n",
    "    # STEP 6: Process long-term embeddings using transformer learners\n",
    "    val_weekly_long_term_embeddings = defaultdict(lambda: defaultdict(dict))\n",
    "    SEQ_LENGTH = 4  # Same as training\n",
    "    \n",
    "    if val_sector_embeddings:\n",
    "        # Initialize long term transformer learners\n",
    "        long_term_g = LongTermTransformerLearner(\n",
    "            input_size=val_sector_embeddings[0]['embeddings'].shape[1],\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            lookback_weeks=SEQ_LENGTH\n",
    "        )\n",
    "        \n",
    "        long_term_a = LongTermTransformerLearner(\n",
    "            input_size=next(iter(val_attentive_embeddings.values())).shape[1],\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            lookback_weeks=SEQ_LENGTH\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get the maximum week index\n",
    "            if val_company_sector_embeddings:\n",
    "                max_weeks = max([max(weekly_data.keys()) for weekly_data in val_company_sector_embeddings.values()])\n",
    "                \n",
    "                # Process each week from SEQ_LENGTH onwards\n",
    "                for current_week in range(SEQ_LENGTH, max_weeks + 1):\n",
    "                    print(f\"Processing validation long-term embeddings for week {current_week}\")\n",
    "                    # Process each company for this week\n",
    "                    for company_idx, weekly_data in val_company_sector_embeddings.items():\n",
    "                        # Check if we have data for this company in this week\n",
    "                        if current_week not in weekly_data:\n",
    "                            continue\n",
    "                            \n",
    "                        # Check if we have enough history\n",
    "                        history_weeks = [w for w in range(current_week - SEQ_LENGTH, current_week)]\n",
    "                        if not all(w in weekly_data for w in history_weeks):\n",
    "                            continue\n",
    "                            \n",
    "                        # Gather the sliding window of embeddings\n",
    "                        graph_window = []\n",
    "                        for w in history_weeks:\n",
    "                            if w in weekly_data:\n",
    "                                graph_emb = torch.tensor(weekly_data[w]).unsqueeze(0)\n",
    "                                graph_window.append(graph_emb)\n",
    "                        \n",
    "                        # Process attentive embeddings if available\n",
    "                        if company_idx in val_attentive_embeddings:\n",
    "                            attentive_seqs = val_attentive_embeddings[company_idx]\n",
    "                            \n",
    "                            if len(attentive_seqs) >= SEQ_LENGTH:\n",
    "                                attentive_window = []\n",
    "                                for i in range(SEQ_LENGTH):\n",
    "                                    seq_idx = len(attentive_seqs) - SEQ_LENGTH + i\n",
    "                                    if seq_idx >= 0 and seq_idx < len(attentive_seqs):\n",
    "                                        att_emb = torch.tensor(attentive_seqs[seq_idx], dtype=torch.float32).unsqueeze(0)\n",
    "                                        attentive_window.append(att_emb)\n",
    "                                \n",
    "                                if len(graph_window) == SEQ_LENGTH and len(attentive_window) == SEQ_LENGTH:\n",
    "                                    tau_G = long_term_g(graph_window)\n",
    "                                    tau_A = long_term_a(attentive_window)\n",
    "                                    \n",
    "                                    # Store the results\n",
    "                                    val_weekly_long_term_embeddings[current_week][company_idx] = {\n",
    "                                        'graph': tau_G.cpu().numpy(),\n",
    "                                        'attentive': tau_A.cpu().numpy()\n",
    "                                    }\n",
    "    \n",
    "    # STEP 7: Create final evaluation data using the latest week\n",
    "    val_data = {\n",
    "        'attentive_embs': [],\n",
    "        'graph_embs': [],\n",
    "        'sector_embs': [],\n",
    "        'returns': [],\n",
    "        'movements': [],\n",
    "        'companies': []\n",
    "    }\n",
    "    \n",
    "    # Use the last week for prediction\n",
    "    if val_weekly_long_term_embeddings:\n",
    "        last_week = max(val_weekly_long_term_embeddings.keys())\n",
    "        print(f\"Using week {last_week} for final evaluation\")\n",
    "        \n",
    "        for company_idx, company_data in val_weekly_long_term_embeddings[last_week].items():\n",
    "            # Get the stock's sector\n",
    "            if company_idx not in stock_sector_map or len(stock_sector_map[company_idx]) == 0:\n",
    "                continue\n",
    "                \n",
    "            sector_id = list(stock_sector_map[company_idx])[0]\n",
    "            \n",
    "            # Check if we have the sector embedding for this week\n",
    "            if sector_id not in val_weekly_inter_sector_embeddings[last_week]:\n",
    "                continue\n",
    "                \n",
    "            # Extract embeddings\n",
    "            attentive_emb = torch.tensor(company_data['attentive'], dtype=torch.float32)\n",
    "            graph_emb = torch.tensor(company_data['graph'], dtype=torch.float32)\n",
    "            sector_emb = val_weekly_inter_sector_embeddings[last_week][sector_id]\n",
    "            \n",
    "            # Get return and movement labels\n",
    "            if company_idx in val_stock_returns_map and val_stock_returns_map[company_idx]:\n",
    "                return_ratio = val_stock_returns_map[company_idx][0]\n",
    "                movement = 1.0 if return_ratio > 0 else 0.0\n",
    "                \n",
    "                # Add to evaluation data\n",
    "                val_data['attentive_embs'].append(attentive_emb)\n",
    "                val_data['graph_embs'].append(graph_emb)\n",
    "                val_data['sector_embs'].append(sector_emb)\n",
    "                val_data['companies'].append(company_idx)\n",
    "                val_data['returns'].append(return_ratio)\n",
    "                val_data['movements'].append(movement)\n",
    "    \n",
    "    # Perform evaluation if we have data\n",
    "    if val_data['returns']:\n",
    "        val_data['attentive_embs'] = torch.stack(val_data['attentive_embs'])\n",
    "        val_data['graph_embs'] = torch.stack(val_data['graph_embs'])\n",
    "        val_data['sector_embs'] = torch.stack(val_data['sector_embs'])\n",
    "        val_data['returns'] = torch.tensor(val_data['returns'], dtype=torch.float32)\n",
    "        val_data['movements'] = torch.tensor(val_data['movements'], dtype=torch.float32)\n",
    "        \n",
    "        # Evaluate\n",
    "        trained_model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Move to device\n",
    "            attentive_embs = val_data['attentive_embs']\n",
    "            graph_embs = val_data['graph_embs']\n",
    "            sector_embs = val_data['sector_embs']\n",
    "            \n",
    "            # Get predictions - using the transformer model\n",
    "            return_preds, movement_preds = trained_model(attentive_embs, graph_embs, sector_embs)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            return_preds_np = return_preds.cpu().numpy()\n",
    "            return_targets_np = val_data['returns'].numpy()\n",
    "            movement_preds_np = movement_preds.cpu().numpy()\n",
    "            movement_targets_np = val_data['movements'].numpy()\n",
    "            \n",
    "            # Correlation for returns\n",
    "            from scipy.stats import spearmanr\n",
    "            corr, _ = spearmanr(return_preds_np, return_targets_np)\n",
    "            \n",
    "            # Accuracy for movement prediction\n",
    "            threshold = 0.5\n",
    "            binary_preds = [1 if p > threshold else 0 for p in movement_preds_np]\n",
    "            accuracy = sum(p == t for p, t in zip(binary_preds, movement_targets_np)) / len(binary_preds)\n",
    "            \n",
    "\n",
    "            \n",
    "            # Calculate MRR metrics\n",
    "            pred_target_pairs = list(zip(return_preds_np, return_targets_np, val_data['companies']))\n",
    "            pred_ranking = sorted(pred_target_pairs, key=lambda x: x[0], reverse=True)\n",
    "            company_to_pred_rank = {company: i+1 for i, (_, _, company) in enumerate(pred_ranking)}\n",
    "            \n",
    "            true_ranking = sorted(\n",
    "                [(target, company) for (_, target, company) in pred_target_pairs],\n",
    "                key=lambda x: x[0], \n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            mrr_values = {}\n",
    "            top_k_values = [5, 10, 20, 30, 100]\n",
    "            \n",
    "            for k in top_k_values:\n",
    "                mrr_at_k = []\n",
    "                for j, (_, company) in enumerate(true_ranking[:min(k, len(true_ranking))]):\n",
    "                    if company in company_to_pred_rank:\n",
    "                        mrr_at_k.append(1.0 / company_to_pred_rank[company])\n",
    "                \n",
    "                mrr_values[k] = sum(mrr_at_k) / len(mrr_at_k) if mrr_at_k else 0\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"\\nValidation Metrics (using transformer model):\")\n",
    "            print(f\"Number of validation samples: {len(return_preds_np)}\")\n",
    "            print(f\"Return prediction correlation: {corr:.4f}\")\n",
    "            print(f\"Movement prediction accuracy: {accuracy:.4f}\")\n",
    "\n",
    "            \n",
    "            print(\"\\nMean Reciprocal Rank (MRR) Metrics:\")\n",
    "            for k in top_k_values:\n",
    "                print(f\"MRR@{k}: {mrr_values[k]:.4f}\")\n",
    "            \n",
    "            # Calculate IRR metrics\n",
    "            irr_values = {}\n",
    "            \n",
    "            for k in top_k_values:\n",
    "                # Sort predictions and get top and bottom k stocks\n",
    "                pred_sorted_indices = np.argsort(return_preds_np)[::-1]\n",
    "                \n",
    "                # Get actual returns for top k and bottom k predicted stocks\n",
    "                if len(pred_sorted_indices) >= k*2:\n",
    "                    top_k_returns = [return_targets_np[i] for i in pred_sorted_indices[:k]]\n",
    "                    bottom_k_returns = [return_targets_np[i] for i in pred_sorted_indices[-k:]]\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    top_k_mean = np.mean(top_k_returns)\n",
    "                    bottom_k_mean = np.mean(bottom_k_returns)\n",
    "                    excess_return = top_k_mean - bottom_k_mean\n",
    "                    \n",
    "                    # Calculate tracking error as standard deviation of return differences\n",
    "                    if len(top_k_returns) > 1:\n",
    "                        tracking_error = np.std(np.array(top_k_returns) - np.array(bottom_k_returns))\n",
    "                        ir = excess_return / tracking_error if tracking_error > 0 else 0\n",
    "                    else:\n",
    "                        ir = 0\n",
    "                        \n",
    "                    irr_values[k] = {\n",
    "                        'ir': ir,\n",
    "                        'top_returns': top_k_mean,\n",
    "                        'bottom_returns': bottom_k_mean,\n",
    "                        'excess_return': excess_return\n",
    "                    }\n",
    "            \n",
    "            # Print IRR metrics\n",
    "            print(\"\\nInformation Ratio (IRR) Metrics:\")\n",
    "            for k in top_k_values:\n",
    "                if k in irr_values:\n",
    "                    print(f\"IRR@{k}: {irr_values[k]['ir']:.4f} (Excess Return: {irr_values[k]['excess_return']:.4f}, \"\n",
    "                          f\"Top-{k} Avg: {irr_values[k]['top_returns']:.4f}, Bottom-{k} Avg: {irr_values[k]['bottom_returns']:.4f})\")\n",
    "                    \n",
    "            return corr, accuracy, mrr_values\n",
    "    else:\n",
    "        print(\"No validation data available for evaluation\")\n",
    "        return None, None, None, None\n",
    "\n",
    "print(\"\\n--- Evaluating on Validation Data with Fresh Embeddings ---\")\n",
    "val_metrics = evaluate_on_validation(model)\n",
    "print(\"\\n--- Evaluation Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef13750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions_csv_with_names(model):\n",
    "    print(\"Generating predictions for each company across all dates...\")\n",
    "    \n",
    "    # Create a mapping from company_id to company name\n",
    "    # Assuming the DataFrame has columns for company ID and name (like 'symbol' or 'company_name')\n",
    "    # If your DataFrame columns are different, adjust accordingly\n",
    "    company_id_to_name = {}\n",
    "    \n",
    "    # Choose one of these approaches based on your DataFrame structure:\n",
    "    \n",
    "    # Option 1: If company identifiers are in the index and names are in a column\n",
    "    if 'company_name' in df_val_date.columns:\n",
    "        for idx, row in df_val_date.reset_index().drop_duplicates(['company_id']).iterrows():\n",
    "            company_id_to_name[row['company_id']] = row['company_name']\n",
    "    \n",
    "    # Option 2: If there's a 'symbol' or 'ticker' column that has readable names\n",
    "    elif 'symbol' in df_val_date.columns:\n",
    "        for idx, row in df_val_date.reset_index().drop_duplicates(['company_id']).iterrows():\n",
    "            company_id_to_name[row['company_id']] = row['symbol']\n",
    "    \n",
    "    # Default: If no name column is found, use the IDs but convert to string\n",
    "    if not company_id_to_name:\n",
    "        print(\"Warning: Could not find company names in the dataset. Using IDs.\")\n",
    "        # Will use str(company_id) as fallback\n",
    "    \n",
    "    predictions_data = {\n",
    "        'date': [],\n",
    "        'company_id': [],\n",
    "        'company_name': [],  # New column for company names\n",
    "        'actual_return': [],\n",
    "        'predicted_return': [],\n",
    "        'movement_actual': [],\n",
    "        'movement_predicted': []\n",
    "    }\n",
    "    \n",
    "    if val_weekly_long_term_embeddings:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for week_idx in sorted(val_weekly_long_term_embeddings.keys()):\n",
    "                try:\n",
    "                    dates = df_val_date.index.unique()\n",
    "                    if week_idx < len(dates):\n",
    "                        current_date = dates[week_idx]\n",
    "                    else:\n",
    "                        current_date = pd.Timestamp(end_date)\n",
    "                except:\n",
    "                    current_date = pd.Timestamp(start_date) + pd.Timedelta(days=7*week_idx)\n",
    "                \n",
    "                for company_idx, company_data in val_weekly_long_term_embeddings[week_idx].items():\n",
    "                    if (company_idx not in stock_sector_map or \n",
    "                        len(stock_sector_map[company_idx]) == 0):\n",
    "                        continue\n",
    "                    \n",
    "                    sector_id = list(stock_sector_map[company_idx])[0]\n",
    "                    if sector_id not in val_weekly_inter_sector_embeddings[week_idx]:\n",
    "                        continue\n",
    "                    \n",
    "                    attentive_emb = torch.tensor(company_data['attentive'], dtype=torch.float32).unsqueeze(0)\n",
    "                    graph_emb = torch.tensor(company_data['graph'], dtype=torch.float32).unsqueeze(0)\n",
    "                    sector_emb = val_weekly_inter_sector_embeddings[week_idx][sector_id].unsqueeze(0)\n",
    "                    \n",
    "                    if company_idx in val_stock_returns_map and len(val_stock_returns_map[company_idx]) > week_idx:\n",
    "                        actual_return = val_stock_returns_map[company_idx][week_idx]\n",
    "                        actual_movement = 1.0 if actual_return > 0 else 0.0\n",
    "                        \n",
    "                        return_pred, movement_pred = model(attentive_emb, graph_emb, sector_emb)\n",
    "                        \n",
    "                        # Get company name from mapping, or use ID as string if not found\n",
    "                        company_name = company_id_to_name.get(company_idx, f\"Company_{company_idx}\")\n",
    "                        \n",
    "                        predictions_data['date'].append(current_date)\n",
    "                        predictions_data['company_id'].append(company_idx)\n",
    "                        predictions_data['company_name'].append(company_name)\n",
    "                        predictions_data['actual_return'].append(actual_return)\n",
    "                        predictions_data['predicted_return'].append(return_pred.item())\n",
    "                        predictions_data['movement_actual'].append(actual_movement)\n",
    "                        predictions_data['movement_predicted'].append(movement_pred.item())\n",
    "    \n",
    "    if predictions_data['date']:\n",
    "        pred_df = pd.DataFrame(predictions_data)\n",
    "        \n",
    "        # Convert company_id to string for better readability\n",
    "        pred_df['company_id'] = pred_df['company_id'].astype(str)\n",
    "        \n",
    "        output_file = 'stock_return_predictions.csv'\n",
    "        pred_df.to_csv(output_file, index=False)\n",
    "        print(f\"Predictions saved to {output_file}\")\n",
    "        \n",
    "        print(\"\\nSample predictions:\")\n",
    "        print(pred_df.head())\n",
    "        \n",
    "        return pred_df\n",
    "    else:\n",
    "        print(\"No prediction data available to save\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d68f2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for each company across all dates...\n",
      "Warning: Could not find company names in the dataset. Using IDs.\n",
      "Predictions saved to stock_return_predictions.csv\n",
      "\n",
      "Sample predictions:\n",
      "                       date company_id company_name  actual_return  \\\n",
      "0 2025-01-07 00:00:00+05:30        131  Company_131       0.023595   \n",
      "1 2025-01-07 00:00:00+05:30        136  Company_136      -0.000654   \n",
      "2 2025-01-07 00:00:00+05:30        266  Company_266      -0.029267   \n",
      "3 2025-01-07 00:00:00+05:30        396  Company_396      -0.008955   \n",
      "4 2025-01-07 00:00:00+05:30        274  Company_274       0.017566   \n",
      "\n",
      "   predicted_return  movement_actual  movement_predicted  \n",
      "0         -0.608756              1.0            0.505033  \n",
      "1         -0.601035              0.0            0.505234  \n",
      "2         -0.597668              0.0            0.505858  \n",
      "3         -0.601886              0.0            0.504720  \n",
      "4         -0.603696              1.0            0.504827  \n",
      "\n",
      "Total predictions generated: 17355\n",
      "Number of unique companies: 445\n",
      "Date range: 2025-01-07 00:00:00+05:30 to 2025-02-28 00:00:00+05:30\n"
     ]
    }
   ],
   "source": [
    "predictions_df = generate_predictions_csv_with_names(model)\n",
    "\n",
    "if predictions_df is not None:\n",
    "    print(f\"\\nTotal predictions generated: {len(predictions_df)}\")\n",
    "    print(f\"Number of unique companies: {predictions_df['company_id'].nunique()}\")\n",
    "    print(f\"Date range: {predictions_df['date'].min()} to {predictions_df['date'].max()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Universal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
